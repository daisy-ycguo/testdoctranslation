<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="ch_configuring-openstack-compute"
         xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:ns5="http://www.w3.org/1999/xhtml"
         xmlns:ns4="http://www.w3.org/2000/svg"
         xmlns:ns3="http://www.w3.org/1998/Math/MathML"
         xmlns:ns="http://docbook.org/ns/docbook">
  <title>Configuring OpenStack Compute</title>

  <para>The OpenStack system has several key projects that are separate
  installations but can work together depending on your cloud needs: OpenStack
  Compute, OpenStack Object Storage, and OpenStack Image Store. There are basic configuration 
  decisions to make, and the <link xlink:href="http://docs.openstack.org/trunk/openstack-compute/install/content/">OpenStack Install Guide</link> 
    covers a basic walkthrough.</para>
  
    <section xml:id="configuring-openstack-compute-basics">
    <?dbhtml stop-chunking?>
    <title>Post-Installation Configuration for OpenStack Compute</title>

    <para>Configuring your Compute installation involves many
      configuration files - the <filename>nova.conf</filename> file,
      the api-paste.ini file, and related Image and Identity
      management configuration files. This section contains the basics
      for a simple multi-node installation, but Compute can be
      configured many ways. You can find networking options and
      hypervisor options described in separate chapters.</para>

    <section xml:id="setting-flags-in-nova-conf-file">
      <title>Setting Configuration Options in the
          <filename>nova.conf</filename> File</title>

      <para>The configuration file <filename>nova.conf</filename> is
        installed in <filename>/etc/nova</filename> by default. A
        default set of options are already configured in
          <filename>nova.conf</filename> when you install manually.   </para>

      <para>Starting with the default file, you must define the following required items in
          <filename>/etc/nova/nova.conf</filename>. The options are described below. You can place
        comments in the <filename>nova.conf</filename> file by entering a new line with a
          <literal>#</literal> sign at the beginning of the line. To see a listing of all possible
        configuration options, refer to <link
          xlink:href="http://wiki.openstack.org/NovaConfigOptions"
          >http://wiki.openstack.org/NovaConfigOptions</link>.</para>

      <table rules="all">
        <caption>Description of <filename>nova.conf</filename> configuration options (not
          comprehensive)</caption>

        <thead>
          <tr>
            <td>Configuration option</td>

            <td>Description</td>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td><literal>sql_connection</literal></td>

            <td>SQL Alchemy connect string (reference); Location of OpenStack
            Compute SQL database</td>
          </tr>

          <tr>
            <td><literal>s3_host</literal></td>

            <td>IP address; Location where OpenStack Compute is hosting the
            objectstore service, which will contain the virtual machine images
            and buckets</td>
          </tr>

          <tr>
            <td><literal>rabbit_host</literal></td>

            <td>IP address; Location of RabbitMQ server</td>
          </tr>

          <tr>
            <td><literal>verbose</literal></td>

            <td>Set to <literal>1</literal> to turn on; Optional but helpful
            during initial setup</td>
          </tr>

          <tr>
            <td><literal>network_manager</literal></td>

            <td><para>Configures how your controller will communicate with
            additional OpenStack Compute nodes and virtual machines.
            Options:</para><itemizedlist>
                <listitem>
                  <para>
                    <literal>nova.network.manager.FlatManager</literal>
                  </para>

                  <para>Simple, non-VLAN networking</para>
                </listitem>

                <listitem>
                  <para>
                    <literal>nova.network.manager.FlatDHCPManager</literal>
                  </para>

                  <para>Flat networking with DHCP</para>
                </listitem>

                <listitem>
                  <para>
                    <literal>nova.network.manager.VlanManager</literal>
                  </para>

                  <para>VLAN networking with DHCP; This is the Default if no
                  network manager is defined here in nova.conf.</para>
                </listitem>
              </itemizedlist></td>
          </tr>

          <tr>
            <td><literal>fixed_range</literal></td>

            <td>IP address/range; Network prefix for the IP network that all
            the projects for future VM guests reside on. Example:
            <literal>192.168.0.0/12</literal></td>
          </tr>

          <tr>
            <td><literal>ec2_host</literal></td>

            <td>IP address; Indicates where the <command>nova-api</command>
            service is installed.</td>
          </tr>

          <tr>
            <td><literal>ec2_url</literal></td>

            <td>Url; Indicates the service for EC2 requests.</td>
          </tr>

          <tr>
            <td><literal>osapi_host</literal></td>

            <td>IP address; Indicates where the <command>nova-api</command>
            service is installed.</td>
          </tr>

          <tr>
            <td><literal>network_size</literal></td>

            <td>Number value; Number of addresses in each private subnet.</td>
          </tr>

          <tr>
            <td><literal>glance_api_servers</literal></td>

            <td>IP and port; Address for Image Service.</td>
          </tr>

          <tr>
            <td><literal>use_deprecated_auth</literal></td>

            <td>If this option is present, the Cactus method of authentication is used with the
              novarc file containing credentials.</td>
          </tr>
        </tbody>
      </table>

      <para>Here is a simple example <filename>nova.conf</filename>
        file for a small private cloud, with all the cloud controller
        services, database server, and messaging server on the same
        server. In this case, CONTROLLER_IP represents the IP address
        of a central server, BRIDGE_INTERFACE represents the bridge
        such as br100, the NETWORK_INTERFACE represents an interface
        to your VLAN setup, and passwords are represented as
        DB_PASSWORD_COMPUTE for your Compute (nova) database password,
        and RABBIT PASSWORD represents the password to your rabbit
        installation.</para>

      <programlisting>[DEFAULT]
verbose=True
instances_path=/var/openstack/compute/instances
auth_strategy=keystone
allow_resize_to_same_host=True
root_helper=sudo /opt/openstack/nova/bin/nova-rootwrap
compute_scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler
dhcpbridge_flagfile=/etc/openstack/compute.conf
fixed_range=192.168.0.0/12
s3_host=${CONTROLLER_IP}
network_manager=nova.network.manager.FlatManager
volume_group=nova-volumes
volume_name_template=volume-%08x
iscsi_helper=tgtadm
osapi_compute_extension=nova.api.openstack.compute.contrib.standard_extensions
my_ip=${MY_IP}
public_interface=${BRIDGE_INTERFACE}
vlan_interface=${NETWORK_INTERFACE}
flat_network_bridge=${BRIDGE_INTERFACE}
flat_interface=${NETWORK_INTERFACE}
sql_connection=postgresql://openstack_compute:${DB_PASSWORD_COMPUTE}@${CONTROLLER_IP}/openstack_compute
libvirt_type=kvm
instance_name_template=instance-%08x
novncproxy_base_url=http://${CONTROLLER_IP}:6080/vnc_auto.html
xvpvncproxy_base_url=http://${CONTROLLER_IP}:6081/console
vncserver_listen=127.0.0.1
vncserver_proxyclient_address=127.0.0.1
api_paste_config=/var/openstack/compute/api-paste.ini
image_service=nova.image.glance.GlanceImageService
ec2_dmz_host=${CONTROLLER_IP}
rabbit_host=${CONTROLLER_IP}
rabbit_password=${RABBIT_PASSWORD}
glance_api_servers=${CONTROLLER_IP}:9292
force_dhcp_release=True
flat_injected=True
connection_type=libvirt
firewall_driver=nova.virt.libvirt.firewall.IptablesFirewallDriver             
            </programlisting>

      <para>Create a “nova” group, so you can set permissions on the
      configuration file:</para>

      <screen><prompt>$</prompt> <userinput>sudo addgroup nova</userinput></screen>

      <para>The <filename>nova.config</filename> file should have its
        owner set to <literal>root:nova</literal>, and mode set to
          <literal>0640</literal>, since the file could contain your
        MySQL server’s username and password. You also want to ensure
        that the <literal>nova</literal> user belongs to the
          <literal>nova</literal> group.</para>

      <screen><prompt>$</prompt> <userinput>sudo usermod -g nova nova</userinput>
<prompt>$</prompt> <userinput>chown -R root:nova /etc/nova</userinput>
<prompt>$</prompt> <userinput>chmod 640 /etc/nova/nova.conf</userinput></screen>
    </section>

    <section xml:id="setting-up-openstack-compute-environment-on-the-compute-node">
      <title>Setting Up OpenStack Compute Environment on the Compute
      Node</title>

      <para>These are the commands you run to ensure the database
        schema is current:</para>

      <screen><prompt>$</prompt> <userinput>nova-manage db sync</userinput></screen>
<para>You also need to populate the database with the network configuration information that Compute obtains from the <filename>nova.conf</filename> file.
</para>
<screen><prompt>$</prompt> <userinput>nova-manage network create <replaceable>&lt;network-label&gt; &lt;project-network&gt; &lt;number-of-networks-in-project&gt; &lt;addresses-in-each-network&gt;</replaceable></userinput>
</screen>

      <para>Here is an example of what this looks like with real values
      entered:</para>

      <screen><prompt>$</prompt> <userinput>nova-manage db sync</userinput>
<prompt>$</prompt> <userinput>nova-manage network create novanet 192.168.0.0/24 1 256</userinput></screen>

      <para>For this example, the number of IPs is <literal>/24</literal>
      since that falls inside the <literal>/16</literal> range that was set in
      <literal>fixed-range</literal> in <filename>nova.conf</filename>.
      Currently, there can only be one network, and this set up would use the
      max IPs available in a <literal>/24</literal>. You can choose values
      that let you use any valid amount that you would like.</para>

      <para>The nova-manage service assumes that the first IP address is your
      network (like 192.168.0.0), that the 2nd IP is your gateway
      (192.168.0.1), and that the broadcast is the very last IP in the range
      you defined (192.168.0.255). If this is not the case you will need to
      manually edit the sql db <literal>networks</literal> table.</para>

      <para>When you run the <command>nova-manage network create</command>
      command, entries are made in the <literal>networks</literal> and
      <literal>fixed_ips</literal> tables. However, one of the networks listed
      in the <literal>networks</literal> table needs to be marked as bridge in
      order for the code to know that a bridge exists. The network in the Nova
      networks table is marked as bridged automatically for Flat
      Manager.</para>
    </section>
    <section xml:id="creating-credentials"><title>Creating Credentials</title>
      <para>The credentials you will use to launch
        instances, bundle images, and all the other assorted
        API functions can be sourced in a single file, such as
        creating one called /creds/openrc. </para>
      <para>Here's an example openrc file you can download from
        the Dashboard in Settings > Project Settings >
        Download RC File. </para>
      <para>
        <programlisting>#!/bin/bash
# *NOTE*: Using the 2.0 *auth api* does not mean that compute api is 2.0.  We
# will use the 1.1 *compute api*
export OS_AUTH_URL=http://50.56.12.206:5000/v2.0
export OS_TENANT_ID=27755fd279ce43f9b17ad2d65d45b75c
export OS_USERNAME=vish
export OS_PASSWORD=$OS_PASSWORD_INPUT
export OS_AUTH_USER=norm
export OS_AUTH_KEY=$OS_PASSWORD_INPUT
export OS_AUTH_TENANT=27755fd279ce43f9b17ad2d65d45b75c
export OS_AUTH_STRATEGY=keystone
</programlisting>
      </para>
      <para>You also may want to enable EC2 access for the
        euca2ools. Here is an example ec2rc file for enabling
        EC2 access with the required credentials.</para>
      <para>
        <programlisting>export NOVA_KEY_DIR=/root/creds/
export EC2_ACCESS_KEY="EC2KEY:USER"
export EC2_SECRET_KEY="SECRET_KEY"
export EC2_URL="http://$NOVA-API-IP:8773/services/Cloud"
export S3_URL="http://$NOVA-API-IP:3333"
export EC2_USER_ID=42 # nova does not use user id, but bundling requires it
export EC2_PRIVATE_KEY=${NOVA_KEY_DIR}/pk.pem
export EC2_CERT=${NOVA_KEY_DIR}/cert.pem
export NOVA_CERT=${NOVA_KEY_DIR}/cacert.pem
export EUCALYPTUS_CERT=${NOVA_CERT} # euca-bundle-image seems to require this set
alias ec2-bundle-image="ec2-bundle-image --cert ${EC2_CERT} --privatekey ${EC2_PRIVATE_KEY} --user 42 --ec2cert ${NOVA_CERT}"
alias ec2-upload-bundle="ec2-upload-bundle -a ${EC2_ACCESS_KEY} -s ${EC2_SECRET_KEY} --url ${S3_URL} --ec2cert ${NOVA_CERT}"</programlisting>
      </para>
      <para>Lastly, here is an example openrc file that works
        with nova client and ec2
        tools.</para><programlisting>export OS_PASSWORD=${ADMIN_PASSWORD:-secrete}
export OS_AUTH_URL=${OS_AUTH_URL:-http://$SERVICE_HOST:5000/v2.0}
export NOVA_VERSION=${NOVA_VERSION:-1.1}
export OS_REGION_NAME=${OS_REGION_NAME:-RegionOne}
export EC2_URL=${EC2_URL:-http://$SERVICE_HOST:8773/services/Cloud}
export EC2_ACCESS_KEY=${DEMO_ACCESS}
export EC2_SECRET_KEY=${DEMO_SECRET}
export S3_URL=http://$SERVICE_HOST:3333
export EC2_USER_ID=42 # nova does not use user id, but bundling requires it
export EC2_PRIVATE_KEY=${NOVA_KEY_DIR}/pk.pem
export EC2_CERT=${NOVA_KEY_DIR}/cert.pem
export NOVA_CERT=${NOVA_KEY_DIR}/cacert.pem
export EUCALYPTUS_CERT=${NOVA_CERT} # euca-bundle-image seems to require this set</programlisting>
      <para>Next, add these credentials to your environment
        prior to running any nova client commands or nova
        commands. </para>
      <literallayout class="monospaced">cat /root/creds/openrc >> ~/.bashrc
source ~/.bashrc                 </literallayout>
    </section>
    <section xml:id="creating-certifications">
      <title>Creating Certificates</title>
      <para>You can create certificates contained within pem
        files using these nova client
        commands, ensuring you have set up your environment variables for the nova client:
        <screen><prompt>#</prompt> <userinput>nova x509-get-root-cert</userinput>
<prompt>#</prompt> <userinput>nova x509-create-cert </userinput></screen>
      </para>
     </section>

    <section xml:id="enabling-access-to-vms-on-the-compute-node">
      <title>Enabling Access to VMs on the Compute Node</title>

      <para>One of the most commonly missed configuration areas is not
      allowing the proper access to VMs. Use the
      <command>euca-authorize</command> command to enable access. Below, you
      will find the commands to allow <command>ping</command> and
      <command>ssh</command> to your VMs :</para>

      <note>
        <para>These commands need to be run as root only if the credentials
        used to interact with <command>nova-api</command> have been put under
        <filename>/root/.bashrc</filename>. If the EC2 credentials have been
        put into another user's <filename>.bashrc</filename> file, then, it is
        necessary to run these commands as the user.</para>
      </note>

      <screen><prompt>$</prompt> <userinput>nova secgroup-add-rule default  icmp -1 -1 0.0.0.0/0</userinput>
<prompt>$</prompt> <userinput>nova secgroup-add-rule default  tcp 22 22 0.0.0.0/0</userinput></screen>

      <para>Another common issue is you cannot ping or SSH to your instances
      after issuing the <command>euca-authorize</command> commands. Something
      to look at is the amount of <command>dnsmasq</command> processes that
      are running. If you have a running instance, check to see that TWO
      <command>dnsmasq</command> processes are running. If not, perform the
      following:</para>

      <screen><prompt>$</prompt> <userinput>sudo killall dnsmasq</userinput>
<prompt>$</prompt> <userinput>sudo service nova-network restart</userinput></screen>

      <para>If you get the <literal>instance not found</literal> message while
      performing the restart, that means the service was not previously
      running. You simply need to start it instead of restarting it : </para>

      <screen><prompt>$</prompt> <userinput>sudo service nova-network start</userinput></screen>
    </section>

    <section xml:id="configuring-multiple-compute-nodes">
      <title>Configuring Multiple Compute Nodes</title>

      <para>If your goal is to split your VM load across more than one server,
      you can connect an additional <command>nova-compute</command> node to a
      cloud controller node. This configuring can be reproduced on multiple
      compute servers to start building a true multi-node OpenStack Compute
      cluster.</para>

      <para>To build out and scale the Compute platform, you spread out
      services amongst many servers. While there are additional ways to
      accomplish the build-out, this section describes adding compute nodes,
      and the service we are scaling out is called
      <command>nova-compute</command>.</para>

      <para>For a multi-node install you only make changes to
          <filename>nova.conf</filename> and copy it to additional
        compute nodes. Ensure each <filename>nova.conf</filename> file
        points to the correct IP addresses for the respective
        services. </para>

      <para>By default, Nova sets the bridge device based on the
        setting in <literal>flat_network_bridge</literal>. Now you can
        edit <filename>/etc/network/interfaces</filename> with the
        following template, updated with your IP information.</para>

      <programlisting># The loopback network interface
auto lo
    iface lo inet loopback

# The primary network interface
auto br100
iface br100 inet static
    bridge_ports    eth0
    bridge_stp      off
    bridge_maxwait  0
    bridge_fd       0
    address <replaceable>xxx.xxx.xxx.xxx</replaceable>
    netmask <replaceable>xxx.xxx.xxx.xxx</replaceable>
    network <replaceable>xxx.xxx.xxx.xxx</replaceable>
    broadcast <replaceable>xxx.xxx.xxx.xxx</replaceable>
    gateway <replaceable>xxx.xxx.xxx.xxx</replaceable>
    # dns-* options are implemented by the resolvconf package, if installed
    dns-nameservers <replaceable>xxx.xxx.xxx.xxx</replaceable>
</programlisting>

      <para>Restart networking:</para>

      <screen><prompt>$</prompt> <userinput>sudo service networking restart</userinput></screen>

      <para>With <filename>nova.conf</filename> updated and networking set,
      configuration is nearly complete. First, bounce the relevant services to
      take the latest updates:</para>

      <screen><prompt>$</prompt> <userinput>sudo service libvirtd restart</userinput>
$ <userinput>sudo service nova-compute restart</userinput></screen>

      <para>To avoid issues with KVM and permissions with Nova, run the
      following commands to ensure we have VM's that are running
      optimally:</para>

      <screen><prompt>#</prompt> <userinput>chgrp kvm /dev/kvm</userinput>
<prompt>#</prompt> <userinput>chmod g+rwx /dev/kvm</userinput></screen>

      <para>If you want to use the 10.04 Ubuntu Enterprise Cloud images that
      are readily available at
      http://uec-images.ubuntu.com/releases/10.04/release/, you may run into
      delays with booting. Any server that does not have
      <command>nova-api</command> running on it needs this iptables entry so
      that UEC images can get metadata info. On compute nodes, configure the
      iptables with this next step:</para>

      <screen><prompt>#</prompt> <userinput>iptables -t nat -A PREROUTING -d 169.254.169.254/32 -p tcp -m tcp --dport 80 -j DNAT --to-destination <replaceable>$NOVA_API_IP</replaceable>:8773</userinput></screen>

      <para>Lastly, confirm that your compute node is talking to your cloud
      controller. From the cloud controller, run this database query:</para>

      <screen><prompt>$</prompt> <userinput>mysql -u<replaceable>$MYSQL_USER</replaceable> -p<replaceable>$MYSQL_PASS</replaceable> nova -e 'select * from services;'</userinput></screen>

      <para>In return, you should see something similar to this:</para>

      <screen><computeroutput>+---------------------+---------------------+------------+---------+----+----------+----------------+-----------+--------------+----------+-------------------+
| created_at          | updated_at          | deleted_at | deleted | id | host     | binary         | topic     | report_count | disabled | availability_zone |
+---------------------+---------------------+------------+---------+----+----------+----------------+-----------+--------------+----------+-------------------+
| 2011-01-28 22:52:46 | 2011-02-03 06:55:48 | NULL       |       0 |  1 | osdemo02 | nova-network   | network   |        46064 |        0 | nova              |
| 2011-01-28 22:52:48 | 2011-02-03 06:55:57 | NULL       |       0 |  2 | osdemo02 | nova-compute   | compute   |        46056 |        0 | nova              |
| 2011-01-28 22:52:52 | 2011-02-03 06:55:50 | NULL       |       0 |  3 | osdemo02 | nova-scheduler | scheduler |        46065 |        0 | nova              |
| 2011-01-29 23:49:29 | 2011-02-03 06:54:26 | NULL       |       0 |  4 | osdemo01 | nova-compute   | compute   |        37050 |        0 | nova              |
| 2011-01-30 23:42:24 | 2011-02-03 06:55:44 | NULL       |       0 |  9 | osdemo04 | nova-compute   | compute   |        28484 |        0 | nova              |
| 2011-01-30 21:27:28 | 2011-02-03 06:54:23 | NULL       |       0 |  8 | osdemo05 | nova-compute   | compute   |        29284 |        0 | nova              |
+---------------------+---------------------+------------+---------+----+----------+----------------+-----------+--------------+----------+-------------------+</computeroutput>       </screen>

      <para>You can see that <literal>osdemo0{1,2,4,5}</literal> are all
      running <command>nova-compute</command>. When you start spinning up
      instances, they will allocate on any node that is running
      <command>nova-compute</command> from this list.</para>
    </section>

    <section xml:id="determining-version-of-compute">
      <title>Determining the Version of Compute</title>

      <para>You can find the version of the installation by using the
      <command>nova-manage</command> command:</para>

      <screen><prompt>$</prompt> <userinput>nova-manage version list</userinput></screen>
    </section>
    </section>

  <section xml:id="general-compute-configuration-overview">
    <title>General Compute Configuration Overview</title>

    <para>Most configuration information is available in the
        <filename>nova.conf</filename> configuration option file. Here
      are some general purpose configuration options that you can use
      to learn more about the configuration option file and the node.
      The configuration file nova.conf is typically stored in
        <filename>/etc/nova/nova.conf</filename>.</para>

    <para>You can use a particular configuration option file by using
      the <literal>option</literal> (<filename>nova.conf</filename>)
      parameter when running one of the nova- services. This inserts
      configuration option definitions from the given configuration
      file name, which may be useful for debugging or performance
      tuning. Here are some general purpose configuration
      options.</para>

    <table rules="all">
      <caption>Description of general purpose
          <filename>nova.conf</filename> configuration
        options</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>my_ip</literal></td>

          <td>None</td>

          <td>IP address; Calculated to contain the host IP address.</td>
        </tr>

        <tr>
          <td><literal>host</literal></td>

          <td>None</td>

          <td>String value; Calculated to contain the name of the node where
          the cloud controller is hosted</td>
        </tr>

        <tr>
          <td><literal>-?, [no]help</literal></td>

          <td>None</td>

          <td>Show this help.</td>
        </tr>

        <tr>
          <td><literal>[no]helpshort</literal></td>

          <td>None</td>

          <td>Show usage only for this module.</td>
        </tr>

        <tr>
          <td><literal>[no]helpxml</literal></td>

          <td>None</td>

          <td>Show this help, but with XML output instead of text</td>
        </tr>
      </tbody>
    </table>

    <para>If you want to maintain the state of all the services, you
      can use the state_path configuration option to indicate a
      top-level directory for storing data related to the state of
      Compute including images if you are using the Compute object
      store. Here are additional configuration options that apply to
      all nova- services.</para>

    <table rules="all">
      <caption>Description of <filename>nova.conf</filename>
        configuration options for all services</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>state_path</literal></td>

          <td><filename>/Users/username/p/nova/nova/../</filename></td>

          <td>Directory path; Top-level directory for maintaining nova's
          state.</td>
        </tr>

        <tr>
          <td><literal>periodic_interval</literal></td>

          <td><literal>60</literal></td>

          <td>Integer value; Seconds between running periodic tasks.</td>
        </tr>

        <tr>
          <td><literal>report_interval</literal></td>

          <td><literal>10</literal></td>

          <td>Integer value; Seconds between nodes reporting state to the data
          store.</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section xml:id="sample-nova-configuration-files">
    <title>Example <filename>nova.conf</filename> Configuration Files</title>

    <para>The following sections describe many of the configuration
      option settings that can go into the
        <filename>nova.conf</filename> files. These need to be copied
      to each compute node. Here are some sample
        <filename>nova.conf</filename> files that offer examples of
      specific configurations.</para>

    <simplesect>
      <title>Configuration using KVM, FlatDHCP, MySQL, Glance, LDAP, and
      optionally sheepdog, API is EC2</title>

      <para>From <link
      xlink:href="http://wikitech.wikimedia.org/view/OpenStack#On_the_controller_and_all_compute_nodes.2C_configure_.2Fetc.2Fnova.2Fnova.conf">wikimedia.org</link>,
      used with permission. Where you see parameters passed in, it's likely an
      IP address you need.</para>

      <programlisting>
# configured using KVM, FlatDHCP, MySQL, Glance, LDAP, and optionally sheepdog, API is EC2
verbose
daemonize=1
logdir=/var/log/nova
state_path=/var/lib/nova
lock_path=/var/lock/nova
sql_connection=mysql://$nova_db_user:$nova_db_pass@$nova_db_host/$nova_db_name
image_service=nova.image.glance.GlanceImageService
s3_host=$nova_glance_host
glance_api_servers=$nova_glance_host
rabbit_host=$nova_rabbit_host
network_host=$nova_network_host
ec2_url=http://$nova_api_host:8773/services/Cloud
libvirt_type=kvm
dhcpbridge=/usr/bin/nova-dhcpbridge
flat_network_bridge=br100
network_manager=nova.network.manager.FlatDHCPManager
flat_interface=$nova_network_flat_interface
public_interface=$nova_network_public_interface
routing_source_ip=$nova_network_public_ip
ajax_console_proxy_url=$nova_ajax_proxy_url
volume_driver=nova.volume.driver.SheepdogDriver
auth_driver=nova.auth.ldapdriver.LdapDriver
ldap_url=ldap://$nova_ldap_host
ldap_password=$nova_ldap_user_pass
ldap_user_dn=$nova_ldap_user_dn
ldap_user_unit=people
ldap_user_subtree=ou=people,$nova_ldap_base_dn
ldap_project_subtree=ou=groups,$nova_ldap_base_dn
role_project_subtree=ou=groups,$nova_ldap_base_dn
ldap_cloudadmin=cn=cloudadmins,ou=groups,$nova_ldap_base_dn
ldap_itsec=cn=itsec,ou=groups,$nova_ldap_base_dn
ldap_sysadmin=cn=sysadmins,$nova_ldap_base_dn
ldap_netadmin=cn=netadmins,$nova_ldap_base_dn
ldap_developer=cn=developers,$nova_ldap_base_dn
            </programlisting>

      <figure xml:id="Nova_conf_KVM_LDAP">
        <title>KVM, FlatDHCP, MySQL, Glance, LDAP, and optionally
        sheepdog</title>

        <mediaobject>
          <imageobject role="html">
            <imagedata fileref="figures/SCH_5003_V00_NUAC-Network_mode_KVM_LDAP_OpenStack.png"
                       scale="60"/>
          </imageobject>
        </mediaobject>
      </figure>
    </simplesect>

    <simplesect>
      <title>KVM, Flat, MySQL, and Glance, OpenStack or EC2 API</title>

      <para>This example <filename>nova.conf</filename> file is from an
      internal Rackspace test system used for demonstrations.</para>

      <programlisting>
# configured using KVM, Flat, MySQL, and Glance, API is OpenStack (or EC2)
daemonize=1
dhcpbridge_flagfile=/etc/nova/nova.conf
dhcpbridge=/usr/bin/nova-dhcpbridge
flat_network_bridge=br100
lock_path=/var/lock/nova
logdir=/var/log/nova
state_path=/var/lib/nova
verbose
network_manager=nova.network.manager.FlatManager
sql_connection=mysql://$nova_db_user:$nova_db_pass@$nova_db_host/$nova_db_name
osapi_host=$nova_api_host
rabbit_host=$rabbit_api_host
ec2_host=$nova_api_host
image_service=nova.image.glance.GlanceImageService
glance_api_servers=$nova_glance_host

# first 3 octets of the network your volume service is on, substitute with real numbers
iscsi_ip_prefix=nnn.nnn.nnn 
        </programlisting>

      <figure xml:id="Nova_conf_KVM_Flat">
        <title>KVM, Flat, MySQL, and Glance, OpenStack or EC2 API</title>

        <mediaobject>
          <imageobject role="html">
            <imagedata fileref="figures/SCH_5004_V00_NUAC-Network_mode_KVM_Flat_OpenStack.png"
                       scale="60"/>
          </imageobject>
        </mediaobject>
      </figure>
    </simplesect>

    <simplesect>
      <title>XenServer 5.6, Flat networking, MySQL, and Glance, OpenStack
      API</title>

      <para>This example <filename>nova.conf</filename> file is from an
      internal Rackspace test system.</para>

      <programlisting>
verbose
nodaemon
sql_connection=mysql://root:&lt;password&gt;@127.0.0.1/nova
network_manager=nova.network.manager.FlatManager
image_service=nova.image.glance.GlanceImageService
flat_network_bridge=xenbr0
connection_type=xenapi
xenapi_connection_url=https://&lt;XenServer IP&gt;
xenapi_connection_username=root
xenapi_connection_password=supersecret
rescue_timeout=86400
allow_admin_api=true
xenapi_inject_image=false
use_ipv6=true

# To enable flat_injected, currently only works on Debian-based systems
flat_injected=true
ipv6_backend=account_identifier
ca_path=./nova/CA

# Add the following to your conf file if you're running on Ubuntu Maverick
xenapi_remap_vbd_dev=true
            </programlisting>

      <figure xml:id="Nova_conf_XEN_Flat">
        <title>KVM, Flat, MySQL, and Glance, OpenStack or EC2 API</title>

        <mediaobject>
          <imageobject role="html">
            <imagedata fileref="figures/SCH_5005_V00_NUAC-Network_mode_XEN_Flat_OpenStack.png"
                       scale="60"/>
          </imageobject>
        </mediaobject>
      </figure>
    </simplesect>
  </section>

  <section xml:id="configuring-logging">
    <title>Configuring Logging</title>

    <para>You can use <filename>nova.conf</filename> configuration
      options to indicate where Compute will log events, the level of
      logging, and customize log formats.</para>

    <table rules="all">
      <caption>Description of nova.conf configuration options for
        logging</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>logdir</literal></td>

          <td><literal>/var/logs/nova</literal></td>

          <td>Directory path; Output to a per-service log file in the named
          directory.</td>
        </tr>

        <tr>
          <td><literal>logfile</literal></td>

          <td>Based on service name.</td>

          <td>File name; Output to named file.</td>
        </tr>

        <tr>
          <td><literal>[no]use_syslog</literal></td>

          <td><literal>False</literal></td>

          <td>Output to syslog using their file naming system.</td>
        </tr>

        <tr>
          <td><literal>default_log_levels</literal></td>

          <td><literal>amqplib=WARN,sqlalchemy=WARN,eventlet.wsgi.server=WARN</literal></td>

          <td>Pair of named loggers and level of message to be logged; List of
          logger=LEVEL pairs</td>
        </tr>

        <tr>
          <td><literal>verbose</literal></td>

          <td><literal>False</literal></td>

          <td>Set to 1 or true to turn on; Shows debug output - optional but
          helpful during initial setup.</td>
        </tr>
      </tbody>
    </table>

    <para>To customize log formats for OpenStack Compute, use these
      configuration option settings.</para>

    <table rules="all">
      <caption>Description of nova.conf configuration options for
        customized log formats</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>logging_context_format_string</literal></td>

          <td><literal>%(asctime)s %(levelname)s %(name)s [%(request_id)s
          %(user)s %(project)s] %(message)s</literal></td>

          <td>The format string to use for log messages with additional
          context.</td>
        </tr>

        <tr>
          <td><literal>logging_debug_format_suffix</literal></td>

          <td><literal>from %(processName)s (pid=%(process)d) %(funcName)s
          %(pathname)s:%(lineno)d</literal></td>

          <td>The data to append to the log format when level is DEBUG</td>
        </tr>

        <tr>
          <td><literal>logging_default_format_string</literal></td>

          <td><literal>%(asctime)s %(levelname)s %(name)s [-]
          %(message)s</literal></td>

          <td>The format string to use for log messages without context.</td>
        </tr>

        <tr>
          <td><literal>logging_exception_prefix</literal></td>

          <td><literal>(%(name)s): TRACE:</literal></td>

          <td>String value; Prefix each line of exception output with this
          format.</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section xml:id="configuring-hypervisors">
    <title>Configuring Hypervisors</title>

    <para>OpenStack Compute requires a hypervisor and supports several
      hypervisors and virtualization standards. Configuring and
      running OpenStack Compute to use a particular hypervisor takes
      several installation and configuration steps. The
        <literal>libvirt_type</literal> configuration option indicates
      which hypervisor will be used. Refer to <xref
        linkend="hypervisor-configuration-basics"> Hypervisor
        configuration basics </xref> for more details.</para>
  </section>

  <section xml:id="configuring-authentication-authorization">
    <title>Configuring Authentication and Authorization</title>

    <para>There are different methods of authentication for the
      OpenStack Compute project, including no authentication,
      keystone, or deprecated (which uses nova-manage commands to
      create users). With additional configuration, you can use the
      OpenStack Identity Service, code-named Keystone. Refer to
      <xref linkend="ch_identity_mgmt">Identity Management</xref> for additional information.</para>

    <table rules="all">
      <caption>Description of <filename>nova.conf</filename>
        configuration options for Authentication</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>auth_driver</literal></td>

          <td><literal>nova.auth.dbdriver.DbDriver</literal></td>

          <td><para>String value; Name of the driver for authentication</para>
          <itemizedlist>
              <listitem>
                <para><literal>nova.auth.dbdriver.DbDriver</literal> - Default
                setting, uses credentials stored in zip file, one per
                project.</para>
              </listitem>

              <listitem>
                <para><literal>nova.auth.ldapdriver.FakeLdapDriver</literal> -
                create a replacement for this driver supporting other backends
                by creating another class that exposes the same public
                methods.</para>
              </listitem>
            </itemizedlist></td>
        </tr>

        <tr>
          <td><literal>auth_strategy</literal></td>

          <td><literal>keystone</literal></td>

          <td><para>The strategy to use for auth. Supports noauth, keystone, and
              deprecated.</para></td>
        </tr>
      </tbody>
    </table>

    <table rules="all">
      <caption>Description of <filename>nova.conf</filename>
        configuration options for customizing roles in deprecated
        auth</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>allowed_roles</literal></td>

          <td><literal>cloudadmin,itsec,sysadmin,netadmin,developer</literal></td>

          <td>Comma separated list; Allowed roles for project</td>
        </tr>

        <tr>
          <td><literal>global_roles</literal></td>

          <td><literal>cloudadmin,itsec</literal></td>

          <td>Comma separated list; Roles that apply to all projects</td>
        </tr>

        <tr>
          <td><literal>superuser_roles</literal></td>

          <td><literal>cloudadmin</literal></td>

          <td>Comma separated list; Roles that ignore authorization checking
          completely</td>
        </tr>
      </tbody>
    </table>

    <table rules="all">
      <caption>Description of <filename>nova.conf</filename>
        configuration options for credentials in deprecated
        auth</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>credentials_template</literal></td>

          <td>None</td>

          <td>Directory; Template for creating users' RC file</td>
        </tr>

        <tr>
          <td><literal>credential_rc_file</literal></td>

          <td><literal>%src</literal></td>

          <td>File name; File name of rc in credentials zip</td>
        </tr>

        <tr>
          <td><literal>credential_cert_file</literal></td>

          <td><literal>cert.pem</literal></td>

          <td>File name; File name of certificate in credentials zip</td>
        </tr>

        <tr>
          <td><literal>credential_key_file</literal></td>

          <td><literal>pk.pem</literal></td>

          <td>File name; File name of rc in credentials zip</td>
        </tr>

        <tr>
          <td><literal>vpn_client_template</literal></td>

          <td><literal>nova/cloudpipe/client/ovpn.template</literal></td>

          <td>Directory; Refers to where the template lives for creating users
          vpn file</td>
        </tr>

        <tr>
          <td><literal>credential_vpn_file</literal></td>

          <td><literal>nova-vpn.conf</literal></td>

          <td>File name; Filename of certificate in
          <filename>credentials.zip</filename></td>
        </tr>
      </tbody>
    </table>

    <table rules="all">
      <caption>Description of <filename>nova.conf</filename>
        configuration options for CA (Certificate Authority)</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>keys_path</literal></td>

          <td><literal><replaceable>$state_path</replaceable>/keys</literal></td>

          <td>Directory; Where Nova keeps the keys</td>
        </tr>

        <tr>
          <td><literal>ca_file</literal></td>

          <td><literal>cacert.pem</literal></td>

          <td>File name; File name of root CA</td>
        </tr>

        <tr>
          <td><literal>crl_file</literal></td>

          <td><literal>crl.pem</literal></td>

          <td>File name; File name of Certificate Revocation List</td>
        </tr>

        <tr>
          <td><literal>key_file</literal></td>

          <td><literal>private/cakey.pem</literal></td>

          <td>File name; File name of private key</td>
        </tr>

        <tr>
          <td><literal>use_project_ca</literal></td>

          <td><literal>false</literal></td>

          <td>True or false; Indicates whether to use a CA for each project;
          false means CA is not used for each project</td>
        </tr>

        <tr>
          <td><literal>project_cert_subject</literal></td>

          <td><literal>/C=US/ST=California/L=MountainView/O=AnsoLabs/OU=NovaDev/CN=proje
          ct-ca-%s-%s</literal></td>

          <td>String; Subject for certificate for projects, %s for project,
          timestamp</td>
        </tr>

        <tr>
          <td><literal>user_cert_subject</literal></td>

          <td><literal>/C=US/ST=California/L=MountainView/O=AnsoLabs/OU=NovaDev/CN=%s-%s-%s</literal></td>

          <td>String; Subject for certificate for users, %s for project,
          users, timestamp</td>
        </tr>

        <tr>
          <td><literal>vpn_cert_subject</literal></td>

          <td><literal>/C=US/ST=California/L=MountainView/O=AnsoLabs/OU=NovaDev/CN=project-vpn-%s-%s</literal></td>

          <td>String; Subject for certificate for vpns, %s for project,
          timestamp</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section xml:id="configuring-compute-to-use-ipv6-addresses">
    <title>Configuring Compute to use IPv6 Addresses</title>

    <para>You can configure Compute to use both IPv4 and IPv6 addresses for
    communication by putting it into a IPv4/IPv6 dual stack mode. In IPv4/IPv6
    dual stack mode, instances can acquire their IPv6 global unicast address
    by stateless address autoconfiguration mechanism [RFC 4862/2462].
    IPv4/IPv6 dual stack mode works with VlanManager and FlatDHCPManager
    networking modes, though floating IPs are not supported in the Bexar
    release. In VlanManager, different 64bit global routing prefix is used for
    each project. In FlatDHCPManager, one 64bit global routing prefix is used
    for all instances. The Cactus release includes support for the FlatManager
    networking mode with a required database migration.</para>

    <para>This configuration has been tested on Ubuntu 10.04 with VM images
    that have IPv6 stateless address autoconfiguration capability (must use
    EUI-64 address for stateless address autoconfiguration), a requirement for
    any VM you want to run with an IPv6 address. Each node that executes a
    nova- service must have python-netaddr and radvd installed.</para>

    <para>On all nova-nodes, install python-netaddr:</para>

    <para><literallayout class="monospaced">sudo apt-get install -y python-netaddr</literallayout></para>

    <para>On all nova-network nodes install radvd and configure IPv6
    networking:</para>

    <literallayout class="monospaced">sudo apt-get install -y radvd 
sudo bash -c "echo 1 &gt; /proc/sys/net/ipv6/conf/all/forwarding" 
sudo bash -c "echo 0 &gt; /proc/sys/net/ipv6/conf/all/accept_ra"</literallayout>

    <para>Edit the <filename>nova.conf</filename> file on all nodes to
      set the use_ipv6 configuration option to True. Restart all
      nova- services.</para>

    <para>When using the command 'nova-manage network create' you can add a
    fixed range for IPv6 addresses. You must specify public or private after
    the create parameter.</para>

    <para><literallayout class="monospaced">nova-manage network create public fixed_range num_networks network_size [vlan_start] [vpn_start] [fixed_range_v6]</literallayout></para>

    <para>You can set IPv6 global routing prefix by using the fixed_range_v6
    parameter. The default is: fd00::/48. When you use FlatDHCPManager, the
    command uses the original value of fixed_range_v6. When you use
    VlanManager, the command creates prefixes of subnet by incrementing subnet
    id. Guest VMs uses this prefix for generating their IPv6 global unicast
    address.</para>

    <para>Here is a usage example for VlanManager:</para>

    <para><literallayout class="monospaced">nova-manage network create public 10.0.1.0/24 3 32 100 1000 fd00:1::/48 </literallayout></para>

    <para>Here is a usage example for FlatDHCPManager:</para>

    <para><literallayout class="monospaced">nova-manage network create public 10.0.2.0/24 3 32 0 0 fd00:1::/48 </literallayout></para>

    <para>Note that [vlan_start] and [vpn_start] parameters are not used by
    FlatDHCPManager.</para>

    <table rules="all">
      <caption>Description of <filename>nova.conf</filename>
        configuration options for configuring IPv6</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>use_ipv6</literal></td>

          <td><literal>false</literal></td>

          <td>Set to 1 or true to turn on; Determines whether to use IPv6
          network addresses</td>
        </tr>

        <tr>
          <td><literal>flat_injected</literal></td>

          <td><literal>false</literal></td>

          <td>Cactus only:Indicates whether Compute (Nova) should use attempt
          to inject IPv6 network configuration information into the guest. It
          attempts to modify <filename>/etc/network/interfaces</filename> and
          currently only works on Debian-based systems.</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section xml:id="configuring-compute-to-use-the-image-service">
    <title>Configuring Image Service and Storage for Compute</title>

    <para>Diablo uses <application>Glance</application> for storing
      and retrieving images. After you have installed a
        <application>Glance</application> server, you can configure
        <command>nova-compute</command> to use
        <application>Glance</application> for image storage and
      retrieval. You must ensure the
        <literal>image_service</literal> configuration option is
      defined with the <application>Glance</application> service :
        '<literal>nova.image.glance.GlanceImageService</literal>' uses
        <application>Glance</application> to store and retrieve images
      for OpenStack Compute.</para>

    <table rules="all">
      <caption>Description of <filename>nova.conf</filename>
        configuration options for the Glance image service and
        storage</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>image_service</literal></td>

          <td><literal>nova.image.local.GlanceImageService</literal></td>

          <td><para>The service to use for retrieving and searching for
          images. Images must be registered using euca2ools. Options:
          </para><itemizedlist>
              <listitem>
                <para><literal>nova.image.s3.S3ImageService</literal></para>

                <para>S3 backend for the Image Service.</para>
              </listitem>

              <listitem>
                <para><emphasis
                role="bold"><literal>nova.image.glance.GlanceImageService</literal></emphasis></para>

                <para>Glance back end for storing and retrieving images; See
                <link
                xlink:href="http://glance.openstack.org">http://glance.openstack.org</link>
                for more info.</para>
              </listitem>
            </itemizedlist></td>
        </tr>

        <tr>
          <td><literal>glance_api_servers</literal></td>

          <td><literal><replaceable>$my_ip</replaceable>:9292</literal></td>

          <td>List of Glance API hosts. Each item may contain a host (or IP
          address) and port of an OpenStack Compute Image Service server
          (project's name is Glance)</td>
        </tr>

        <tr>
          <td><literal>s3_dmz</literal></td>

          <td><literal><replaceable>$my_ip</replaceable></literal></td>

          <td>IP address; For instances internal IP (a DMZ is shorthand for a
          demilitarized zone)</td>
        </tr>

        <tr>
          <td><literal>s3_host</literal></td>

          <td><literal><replaceable>$my_ip</replaceable></literal></td>

          <td>IP address: IP address of the S3 host for infrastructure.
          Location where OpenStack Compute is hosting the objectstore service,
          which will contain the virtual machine images and buckets.</td>
        </tr>

        <tr>
          <td><literal>s3_port</literal></td>

          <td><literal>3333</literal></td>

          <td>Integer value; Port where S3 host is running</td>
        </tr>

        <tr>
          <td><literal>use_s3</literal></td>

          <td><literal>true</literal></td>

          <td>Set to 1 or true to turn on; Determines whether to get images
          from s3 or use a local copy</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section xml:id="configuring-live-migrations">
    <title>Configuring Live Migrations</title>

    <para>The live migration feature is useful when you need to upgrade or
    installing patches to hypervisors/BIOS and you need the machines to keep
    running. For example, when one of HDD volumes RAID or one of bonded NICs
    is out of order. Also for regular periodic maintenance, you may need to
    migrate VM instances. When many VM instances are running on a specific
    physical machine, you can redistribute the high load. Sometimes when VM
    instances are scattered, you can move VM instances to a physical machine
    to arrange them more logically.</para>

    <para><emphasis role="bold">Environments</emphasis> <itemizedlist>
        <listitem>
          <para><emphasis role="bold">OS:</emphasis> Ubuntu 10.04/10.10 for
          both instances and host.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Shared storage:</emphasis>
          NOVA-INST-DIR/instances/ has to be mounted by shared storage (tested
          using NFS).</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Instances:</emphasis> Instance can be
          migrated with ISCSI based volumes</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Hypervisor:</emphasis> KVM with
          libvirt</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">(NOTE1)</emphasis>
          "NOVA-INST-DIR/instance" is expected that vm image is put on to. see
          "flags.instances_path" in nova.compute.manager for the default
          value</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">(NOTE2)</emphasis> This feature is admin
          only, since nova-manage is necessary.</para>
        </listitem>
      </itemizedlist></para>

    <para><emphasis role="bold">Sample Nova Installation before
    starting</emphasis> <itemizedlist>
        <listitem>
          <para>Prepare 3 servers at least, lets say, HostA, HostB and
          HostC</para>
        </listitem>

        <listitem>
          <para>nova-api/nova-network/nova-volume/nova-objectstore/
          nova-scheduler(and other daemon) are running on HostA.</para>
        </listitem>

        <listitem>
          <para>nova-compute is running on both HostB and HostC.</para>
        </listitem>

        <listitem>
          <para>HostA export NOVA-INST-DIR/instances, HostB and HostC mount
          it.</para>
        </listitem>

        <listitem>
          <para>To avoid any confusion, NOVA-INST-DIR is same at
          HostA/HostB/HostC("NOVA-INST-DIR" shows top of install dir).</para>
        </listitem>

        <listitem>
          <para>HostA export NOVA-INST-DIR/instances, HostB and HostC mount
          it.</para>
        </listitem>
      </itemizedlist></para>

    <para><emphasis role="bold">Pre-requisite configurations</emphasis></para>

    <para><orderedlist>
        <listitem>
          <para>Configure /etc/hosts, Make sure 3 Hosts can do name-resolution
          with each other. Ping with each other is better way to test.</para>

          <literallayout class="monospaced">                      
ping HostA
ping HostB
ping HostC
                            </literallayout>
        </listitem>

        <listitem>
          <para>Configure NFS at HostA by adding below to /etc/exports</para>

          <literallayout class="monospaced">NOVA-INST-DIR/instances HostA/255.255.0.0(rw,sync,fsid=0,no_root_squash</literallayout>

          <para>Change "255.255.0.0" appropriate netmask, which should include
          HostB/HostC. Then restart nfs server.</para>

          <literallayout class="monospaced">
/etc/init.d/nfs-kernel-server restart
/etc/init.d/idmapd restart
                            </literallayout>
        </listitem>

        <listitem>
          <para>Configure NFS at HostB and HostC by adding below to
          /etc/fstab</para>

          <literallayout class="monospaced">HostA:/ DIR nfs4 defaults 0 0</literallayout>

          <para>Then mount, check exported directory can be mounted.</para>

          <literallayout class="monospaced">mount -a -v</literallayout>

          <para>If fail, try this at any hosts.</para>

          <literallayout class="monospaced">iptables -F</literallayout>

          <para>Also, check file/daemon permissions. We expect any nova
          daemons are running as root.</para>

          <literallayout class="monospaced">ps -ef | grep nova                            </literallayout>

          <programlisting>root 5948 5904 9 11:29 pts/4 00:00:00 python /opt/nova-2010.4//bin/nova-api
root 5952 5908 6 11:29 pts/5 00:00:00 python /opt/nova-2010.4//bin/nova-objectstore
... (snip)                           </programlisting>

          <para>"<filename><replaceable>NOVA-INST-DIR</replaceable>/instances/</filename>"
          directory can be seen at HostA</para>

          <literallayout class="monospaced">ls -ld <filename><replaceable>NOVA-INST-DIR</replaceable>/instances/</filename></literallayout>

          <programlisting>drwxr-xr-x 2 root root 4096 2010-12-07 14:34 nova-install-dir/instances/                            </programlisting>

          <para>Same check at HostB and HostC</para>

          <literallayout>ls -ld <filename><replaceable>NOVA-INST-DIR</replaceable>/instances/</filename></literallayout>

          <programlisting>drwxr-xr-x 2 root root 4096 2010-12-07 14:34 nova-install-dir/instances/</programlisting>

          <literallayout class="monospaced">df -k</literallayout>

          <programlisting>Filesystem           1K-blocks      Used Available Use% Mounted on
/dev/sda1            921514972   4180880 870523828   1% /
none                  16498340      1228  16497112   1% /dev
none                  16502856         0  16502856   0% /dev/shm
none                  16502856       368  16502488   1% /var/run
none                  16502856         0  16502856   0% /var/lock
none                  16502856         0  16502856   0% /lib/init/rw
HostA:        921515008 101921792 772783104  12% /opt  ( &lt;--- this line is important.)
                            </programlisting>
        </listitem>

        <listitem>
          <para>Libvirt configurations. Modify
          <filename>/etc/libvirt/libvirt.conf</filename>:</para>

          <programlisting>
before : #listen_tls = 0
after : listen_tls = 0

before : #listen_tcp = 1
after : listen_tcp = 1

add: auth_tcp = "none"
                            </programlisting>

          <para>Modify <filename>/etc/init/libvirt-bin.conf</filename></para>

          <programlisting>
before : exec /usr/sbin/libvirtd -d
after : exec /usr/sbin/libvirtd -d -l
                            </programlisting>

          <para>Modify <filename>/etc/default/libvirt-bin</filename></para>

          <programlisting>
before :libvirtd_opts=" -d"
after :libvirtd_opts=" -d -l"
                            </programlisting>

          <para>then, restart libvirt. Make sure libvirt is restarted.</para>

          <literallayout class="monospaced">
stop libvirt-bin &amp;&amp; start libvirt-bin

ps -ef | grep libvirt</literallayout>

          <programlisting>root 1145 1 0 Nov27 ? 00:00:03 /usr/sbin/libvirtd -d -l                           </programlisting>
        </listitem>

        <listitem>
          <para>Configuration option configuration. usually, you do
            not have to configure any configuration options. Below
            chart is only for advanced usage.</para>
        </listitem>
      </orderedlist></para>

    <table rules="all">
      <caption>Description of <filename>nova.conf</filename>
        configuration options for live migration</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>live_migration_retry_count</literal></td>

          <td><literal>30</literal></td>

          <td>Retry count needed in live_migration. Sleep 1sec for each
          retry</td>
        </tr>

        <tr>
          <td><literal>live_migration_uri</literal></td>

          <td><literal>qemu+tcp://%s/system</literal></td>

          <td>Define protocol used by live_migration feature. If you would
          like to use qemu+ssh, change this as described at <link
          xlink:href="http://libvirt.org">http://libvirt.org/</link>.</td>
        </tr>

        <tr>
          <td><literal>live_migration_bandwidth</literal></td>

          <td><literal>0</literal></td>

          <td>Define bandwidth used by live migration.</td>
        </tr>

        <tr>
          <td><literal>live_migration_flag</literal></td>

          <td><literal>VIR_MIGRATE_UNDEFINE_SOURCE,
          VIR_MIGRATE_PEER2PEER</literal></td>

          <td>Define libvirt configuration option for live
            migration.</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section xml:id="configuring-database-connections">
    <title>Configuring Database Connections</title>

    <para>You can configure OpenStack Compute to use any SQLAlchemy-compatible
    database. The database name is 'nova' and entries to it are mostly written
    by the nova-scheduler service, although all the services need to be able
    to update entries in the database. Use these settings to configure the
    connection string for the nova database.</para>

    <table rules="all">
      <caption>Description of nova.conf configuration options for
        database access</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>sql_connection</literal></td>

          <td><literal>sqlite:///<replaceable>$state_path</replaceable>/nova.sqlite</literal></td>

          <td>IP address; Location of OpenStack Compute SQL database</td>
        </tr>

        <tr>
          <td><literal>sql_idle_timeout</literal></td>

          <td><literal>3600</literal></td>

          <td>Integer value; Number of seconds to wait for a database
          connection</td>
        </tr>

        <tr>
          <td><literal>db_backend</literal></td>

          <td><literal>sqlalchemy</literal></td>

          <td>The backend selected for the database connection</td>
        </tr>

        <tr>
          <td><literal>db_driver</literal></td>

          <td><literal>nova.db.api</literal></td>

          <td>The drive to use for database access</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section xml:id="configuring-compute-messaging">
    <title>Configuring the Compute Messaging System</title>

    <para>OpenStack Compute uses an open standard for messaging middleware
    known as AMQP. This messaging middleware enables the OpenStack compute
    services which will exist across multiple servers to talk to each other.
    OpenStack Compute supports two implementations of AMQP:
    <application>RabbitMQ</application> and
    <application>Qpid</application>.</para>

    <section xml:id="configuration-rabbitmq">
      <title>Configuration for RabbitMQ</title>

      <para>OpenStack Compute uses <application>RabbitMQ</application> by
      default. This section discusses the configuration options that are
      relevant when <application>RabbitMQ</application> is used. The
      <literal>rpc_backend</literal> option is not required as long as
      <application>RabbitMQ</application> is the default messaging system.
      However, if it is included the configuration, it must be set to
      <literal>nova.rpc.impl_kombu</literal>.</para>

      <programlisting>rpc_backend=nova.rpc.impl_kombu</programlisting>

      <para>The following tables describe the rest of the options that can be
      used when <application>RabbitMQ</application> is used as the messaging
      system. You can configure the messaging communication for different
      installation scenarios as well as tune RabbitMQ's retries and the size
      of the RPC thread pool.</para>

      <table rules="all">
        <caption>Description of <filename>nova.conf</filename>
          configuration options for Remote Procedure Calls and
          RabbitMQ Messaging</caption>

        <thead>
          <tr>
            <td>Configuration option</td>

            <td>Default</td>

            <td>Description</td>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td><literal>rabbit_host</literal></td>

            <td><literal>localhost</literal></td>

            <td>IP address; Location of <application>RabbitMQ</application>
            installation.</td>
          </tr>

          <tr>
            <td><literal>rabbit_password</literal></td>

            <td><literal>guest</literal></td>

            <td>String value; Password for the
            <application>RabbitMQ</application> server.</td>
          </tr>

          <tr>
            <td><literal>rabbit_port</literal></td>

            <td><literal>5672</literal></td>

            <td>Integer value; Port where <application>RabbitMQ</application>
            server is running/listening.</td>
          </tr>

          <tr>
            <td><literal>rabbit_userid</literal></td>

            <td><literal>guest</literal></td>

            <td>String value; User ID used for
            <application>RabbitMQ</application> connections.</td>
          </tr>

          <tr>
            <td><literal>rabbit_virtual_host</literal></td>

            <td><literal>/</literal></td>

            <td>Location of a virtual <application>RabbitMQ</application>
            installation.</td>
          </tr>
        </tbody>
      </table>

      <table rules="all">
        <caption>Description of <filename>nova.conf</filename>
          configuration options for Tuning RabbitMQ
          Messaging</caption>

        <thead>
          <tr>
            <td>Configuration option</td>

            <td>Default</td>

            <td>Description</td>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td>-<literal>-rabbit_max_retries</literal></td>

            <td><literal>12</literal></td>

            <td>Integer value; <application>RabbitMQ</application> connection
            attempts.</td>
          </tr>

          <tr>
            <td><literal>rabbit-retry-interval</literal></td>

            <td><literal>10</literal></td>

            <td>Integer value: <application>RabbitMQ</application> connection
            retry interval.</td>
          </tr>

          <tr>
            <td><literal>rpc_thread_pool_size</literal></td>

            <td><literal>1024</literal></td>

            <td>Integer value: Size of Remote Procedure Call thread pool.</td>
          </tr>
        </tbody>
      </table>
    </section>

    <section xml:id="configuration-qpid">
      <title>Configuration for Qpid</title>

      <para>This section discusses the configuration options that are relevant
      if <application>Qpid</application> is used as the messaging system for
      OpenStack Compute. <application>Qpid</application> is not the default
      messaging system, so it must be enabled by setting the
      <literal>rpc_backend</literal> option in
      <filename>nova.conf</filename>.</para>

      <programlisting>rpc_backend=nova.rpc.impl_qpid</programlisting>

      <para>This next critical option points the compute nodes to the
      <application>Qpid</application> broker (server). Set
      <literal>qpid_hostname</literal> in <filename>nova.conf</filename> to
      be the hostname where the broker is running.</para>

      <note>
        <para>The -<literal>-qpid_hostname</literal> option accepts a value in
        the form of either a hostname or an IP address.</para>
      </note>

      <programlisting>qpid_hostname=hostname.example.com</programlisting>

      <para>If the <application>Qpid</application> broker is listening on a
      port other than the AMQP default of <literal>5672</literal>, you will
      need to set the <literal>qpid_port</literal> option:</para>

      <programlisting>qpid_port=12345</programlisting>

      <para>If you configure the <application>Qpid</application> broker to
      require authentication, you will need to add a username and password to
      the configuration:</para>

      <programlisting>qpid_username=username
qpid_password=password</programlisting>

      <para>By default, TCP is used as the transport. If you would like to
      enable SSL, set the <literal>qpid_protocol</literal> option:</para>

      <programlisting>qpid_protocol=ssl</programlisting>

      <para>The following table lists the rest of the options used by the Qpid
      messaging driver for OpenStack Compute. It is not common that these
      options are used.</para>

      <table rules="all">
        <caption>Remaining <filename>nova.conf</filename>
          configuration options for Qpid support</caption>

        <thead>
          <tr>
            <td>Configuration option</td>

            <td>Default</td>

            <td>Description</td>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td><literal>qpid_sasl_mechanisms</literal></td>

            <td><literal>(Qpid default)</literal></td>

            <td>String value: A space separated list of acceptable SASL
            mechanisms to use for authentication.</td>
          </tr>

          <tr>
            <td><literal>qpid_reconnect_timeout</literal></td>

            <td><literal>(Qpid default)</literal></td>

            <td>Integer value: The number of seconds to wait before deciding
            that a reconnect attempt has failed.</td>
          </tr>

          <tr>
            <td><literal>qpid_reconnect_limit</literal></td>

            <td><literal>(Qpid default)</literal></td>

            <td>Integer value: The limit for the number of times to reconnect
            before considering the connection to be failed.</td>
          </tr>

          <tr>
            <td><literal>qpid_reconnect_interval_min</literal></td>

            <td><literal>(Qpid default)</literal></td>

            <td>Integer value: Minimum number of seconds between connection
            attempts.</td>
          </tr>

          <tr>
            <td><literal>qpid_reconnect_interval_max</literal></td>

            <td><literal>(Qpid default)</literal></td>

            <td>Integer value: Maximum number of seconds between connection
            attempts.</td>
          </tr>

          <tr>
            <td><literal>qpid_reconnect_interval</literal></td>

            <td><literal>(Qpid default)</literal></td>

            <td>Integer value: Equivalent to setting
            <literal>qpid_reconnect_interval_min</literal> and
            <literal>qpid_reconnect_interval_max</literal> to the same
            value.</td>
          </tr>

          <tr>
            <td><literal>qpid_heartbeat</literal></td>

            <td><literal>5</literal></td>

            <td>Integer value: Seconds between heartbeat messages sent to
            ensure that the connection is still alive.</td>
          </tr>

          <tr>
            <td><literal>qpid_tcp_nodelay</literal></td>

            <td><literal>True</literal></td>

            <td>Boolean value: Disable the Nagle algorithm.</td>
          </tr>
        </tbody>
      </table>
    </section>

    <section xml:id="common-messaging-configuration">
      <title>Common Configuration for Messaging</title>

      <para>This section lists options that are common between both the
      <application>RabbitMQ</application> and <application>Qpid</application>
      messaging drivers.</para>

      <table rules="all">
        <caption>Description of <filename>nova.conf</filename>
          configuration options for Customizing Exchange or Topic
          Names</caption>

        <thead>
          <tr>
            <td>Configuration option</td>

            <td>Default</td>

            <td>Description</td>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td><literal>control_exchange</literal></td>

            <td><literal>nova</literal></td>

            <td>String value; Name of the main exchange to connect to</td>
          </tr>

          <tr>
            <td><literal>ajax_console_proxy_topic</literal></td>

            <td><literal>ajax_proxy</literal></td>

            <td>String value; Topic that the ajax proxy nodes listen on</td>
          </tr>

          <tr>
            <td><literal>console_topic</literal></td>

            <td><literal>console</literal></td>

            <td>String value; The topic console proxy nodes listen on</td>
          </tr>

          <tr>
            <td><literal>network_topic</literal></td>

            <td><literal>network</literal></td>

            <td>String value; The topic network nodes listen on.</td>
          </tr>

          <tr>
            <td><literal>scheduler_topic</literal></td>

            <td><literal>scheduler</literal></td>

            <td>String value; The topic scheduler nodes listen on.</td>
          </tr>

          <tr>
            <td><literal>volume_topic</literal></td>

            <td><literal>volume</literal></td>

            <td>String value; Name of the topic that volume nodes listen
            on</td>
          </tr>
        </tbody>
      </table>
    </section>
  </section>

  <section xml:id="configuring-compute-API">
  <title>Configuring the Compute API</title>
  <simplesect>
    <title>Configuring Compute API password handling</title>

    <para> The OpenStack Compute API allows the user to specify an admin
    password when creating (or rebuilding) a server instance.  If no
    password is specified, a randomly generated password is used. The
    password is returned in the API response.</para>

    <para>In practice, the handling of the admin password depends on the
    hypervisor in use, and may require additional configuration of the
    instance, such as installing an agent to handle the password setting.
    If the hypervisor and instance configuration do not support the
    setting of a password at server create time, then the password
    returned by the create API call will be misleading, since it was
    ignored.
    </para>

    <para>To prevent this confusion, the configuration configuration
        option <literal>enable_instance_password</literal> can be
        used to disable the return of the admin password for
        installations that don't support setting instance
        passwords.</para>

    <table rules="all">
      <caption>Description of nova.conf API related configuration
          options</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>enable_instance_password</literal></td>

          <td><literal>true</literal></td>

          <td>When true, the create and rebuild compute API calls return the server admin password.  When false,
          the server admin password is not included in API responses.</td>
        </tr>

      </tbody>
    </table>

  </simplesect>

  <simplesect>
    <title>Configuring Compute API Rate Limiting</title>

    <para>OpenStack Compute supports API rate limiting for the OpenStack API.
    The rate limiting allows an administrator to configure limits on the type
    and number of API calls that can be made in a specific time
    interval.</para>

    <para>When API rate limits are exceeded, HTTP requests will return a error
    with a status code of 413 "Request entity too large", and will also
    include a 'Retry-After' HTTP header. The response body will include the
    error details, and the delay before the request should be retried.</para>

    <para>Rate limiting is not available for the EC2 API.</para>
  </simplesect>
    <simplesect>
      <title>Specifying Limits</title>

      <para>Limits are specified using five values:</para>

      <itemizedlist>
        <listitem>
          <para>The <emphasis role="bold">HTTP method</emphasis> used in the
          API call, typically one of GET, PUT, POST, or DELETE.</para>
        </listitem>

        <listitem>
          <para>A <emphasis role="bold">human readable URI</emphasis> that is
          used as a friendly description of where the limit is applied.</para>
        </listitem>

        <listitem>
          <para>A <emphasis role="bold">regular expression</emphasis>. The
          limit will be applied to all URI's that match the regular expression
          and HTTP Method.</para>
        </listitem>

        <listitem>
          <para>A <emphasis role="bold">limit value </emphasis> that specifies
          the maximum count of units before the limit takes effect.</para>
        </listitem>

        <listitem>
          <para>An <emphasis role="bold">interval</emphasis> that specifies
          time frame the limit is applied to. The interval can be SECOND,
          MINUTE, HOUR, or DAY.</para>
        </listitem>
      </itemizedlist>

      <para>Rate limits are applied in order, relative to the HTTP method,
      going from least to most specific. For example, although the default
      threshold for POST to */servers is 50 per day, one cannot POST to
      */servers more than 10 times within a single minute because the rate
      limits for any POST is 10/min.</para>
    </simplesect>

    <simplesect>
      <title>Default Limits</title>

      <para>OpenStack compute is normally installed with the following limits
      enabled:</para>

      <table rules="all">
        <caption>Default API Rate Limits</caption>

        <thead>
          <tr>
            <td>HTTP method</td>

            <td>API URI</td>

            <td>API regular expression</td>

            <td>Limit</td>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td>POST</td>

            <td>any URI (*)</td>

            <td>.*</td>

            <td>10 per minute</td>
          </tr>

          <tr>
            <td>POST</td>

            <td>/servers</td>

            <td>^/servers</td>

            <td>50 per day</td>
          </tr>

          <tr>
            <td>PUT</td>

            <td>any URI (*)</td>

            <td>.*</td>

            <td>10 per minute</td>
          </tr>

          <tr>
            <td>GET</td>

            <td>*changes-since*</td>

            <td>.*changes-since.*</td>

            <td>3 per minute</td>
          </tr>

          <tr>
            <td>DELETE</td>

            <td>any URI (*)</td>

            <td>.*</td>

            <td>100 per minute</td>
          </tr>
        </tbody>
      </table>
    </simplesect>

    <simplesect>
      <title>Configuring and Changing Limits</title>

      <para>The actual limits are specified in the file
      <filename>etc/nova/api-paste.ini</filename>, as part of the WSGI
      pipeline.</para>

      <para>To enable limits, ensure the '<literal>ratelimit</literal>' filter
      is included in the API pipeline specification. If the
      '<literal>ratelimit</literal>' filter is removed from the pipeline,
      limiting will be disabled. There should also be a definition for the
      ratelimit filter. The lines will appear as follows:</para>

      <programlisting>

[pipeline:openstack_compute_api_v2]
pipeline = faultwrap authtoken keystonecontext ratelimit osapi_compute_app_v2

[pipeline:openstack_volume_api_v1]
pipeline = faultwrap authtoken keystonecontext ratelimit osapi_volume_app_v1

[filter:ratelimit]
paste.filter_factory = nova.api.openstack.compute.limits:RateLimitingMiddleware.factory

            </programlisting>

      <para>To modify the limits, add a '<literal>limits</literal>'
      specification to the <literal>[filter:ratelimit]</literal> section of
      the file. The limits are specified in the order HTTP method, friendly
      URI, regex, limit, and interval. The following example specifies the
      default rate limiting values:</para>

      <programlisting>

[filter:ratelimit]
paste.filter_factory = nova.api.openstack.compute.limits:RateLimitingMiddleware.factory
limits =("POST", "*", ".*", 10, MINUTE);("POST", "*/servers", "^/servers", 50, DAY);("PUT", "*", ".*", 10, MINUTE);("GET", "*changes-since*", ".*changes-since.*", 3, MINUTE);("DELETE", "*", ".*", 100, MINUTE)

            </programlisting>
    </simplesect>
  </section>
</chapter>
